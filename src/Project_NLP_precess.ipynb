{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "from datetime import date, datetime, time, timedelta\n",
    "from scipy import stats \n",
    "\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "'''$$$$$$$$$$$$$$$$$$$$ Start EDA $$$$$$$$$$$$$$$$$$$$'''\n",
    "def general_explore_file(filepath, show = False):\n",
    "    df = pd.read_csv(filepath)\n",
    "    if show:\n",
    "        explore_df(df)\n",
    "    return df\n",
    "\n",
    "def get_prep_data(filepath, parse_dt = ['Opened', 'Closed'], fix_index = 'CaseID'):\n",
    "    df = pd.read_csv(filepath, parse_dates =parse_dt, infer_datetime_format=True)\n",
    "    if fix_index:\n",
    "        df.set_index(fix_index, inplace = True)\n",
    "    return df\n",
    "    \n",
    "def read_csv_chunks_into_df(file_path, chunk_size, parse_date=False):    \n",
    "    if parse_date:\n",
    "        chunks = pd.read_csv(file_path, parse_dates= ['Opened','Closed','Updated'], \\\n",
    "                             infer_datetime_format=True, chunksize = chunk_size )\n",
    "    else:\n",
    "        chunks = pd.read_csv(file_path, chunksize = chunk_size )\n",
    "    df = pd.concat(chunks) \n",
    "    return df\n",
    "\n",
    "def explore_df(df):\n",
    "    print '*********** Shape of df **************'\n",
    "    print df.shape\n",
    "    features = df.columns.values    \n",
    "    print '********** Number of features ***************'\n",
    "    print len(features) \n",
    "    print '********** Features ***************'\n",
    "    print features\n",
    "    print '******* Head of df ******************'\n",
    "    print df.head()\n",
    "    print '******* Info of df ******************'\n",
    "    print df.info()\n",
    "    print '******** Description of df *****************'\n",
    "    print df.describe()\n",
    "    return\n",
    "\n",
    "def import_data(folder, filename):\n",
    "    '''import the data and set the right datetime '''\n",
    "    folder = folder\n",
    "    filename_original = filename\n",
    "    filepath = folder + filename_original\n",
    "    df = general_explore_file(filepath)\n",
    "\n",
    "    df = df.sort_values('CaseID')\n",
    "    df.set_index('CaseID', inplace = True)\n",
    "\n",
    "    ''' parse dates'''\n",
    "    dt_list = ['Opened','Closed','Updated'] # list of datetime columns\n",
    "    time_format = '%m/%d/%Y %I:%M:%S %p'\n",
    "    dft = parsedate(df, dt_list, time_format) # parse ['Opened','Closed','Updated'] to timedate \n",
    "    return dft\n",
    "    \n",
    "def get_unique(df):\n",
    "    features = df.columns.values\n",
    "    print '********* Number of unique values **********'\n",
    "    for feature in features:\n",
    "        print feature,' ', len(df[feature].unique())\n",
    "    return \n",
    "\n",
    "def get_missing(df):\n",
    "    print '********* Number of missing values **********'\n",
    "    df2 = df.isnull()\n",
    "    features = df.columns.values\n",
    "    for feature in features:\n",
    "        temp = df[df2[feature]]\n",
    "        print feature,' ', len(temp)\n",
    "\n",
    "def drop_na_row(df,feature):\n",
    "    df = df.ix[df[feature].notnull(), :]\n",
    "    return df\n",
    "\n",
    "def get_value_counts(df, feature_list):\n",
    "    for feature in feature_list:\n",
    "        print '************ '+feature+' value counts ***********'\n",
    "        print df[feature].value_counts(dropna = False)\n",
    "    return\n",
    "\n",
    "def parsedate(df, columns, time_format):\n",
    "    for column in columns:\n",
    "        df[column] = pd.to_datetime(df[column], format = time_format)\n",
    "    return df\n",
    "\n",
    "def days_to_minutes(dt):\n",
    "    return  dt.total_seconds()//60#(td.seconds//60)%60\n",
    "\n",
    "def days_to_hours(dt):\n",
    "    hours = dt.total_seconds()/3600#(td.seconds//60)%60\n",
    "    return np.round(hours,1)\n",
    "\n",
    "def get_sorted_category_value(df, category):\n",
    "    ''' returns sorted categorical values based on the mean process_hours '''\n",
    "    df1 = df.copy()\n",
    "    dfm = df1.groupby(category).mean()\n",
    "    dfm = dfm.sort_values('Process_hours')\n",
    "    return dfm.index\n",
    "\n",
    "def category_to_numer_dict(df, category, values):\n",
    "    '''\n",
    "    Change a categorical column to numeric and save the categorical values in a \n",
    "    dictionary for later reference values is sorted list of categorical values\n",
    "    '''\n",
    "    dict = defaultdict(str)\n",
    "    for i,value in enumerate(values):\n",
    "        dict[i] = value # store the categorical values in a dictionary for reference\n",
    "        df.ix[df[category]==value, category] = i\n",
    "    df[category].astype(int, inplace=True)\n",
    "    return dict\n",
    "\n",
    "def category_to_numer_basic(df, category):\n",
    "    '''Change a categorical column to numeric and save the categorical \n",
    "    values in a dictionary for later reference (basic version)'''\n",
    "    values = df[category].unique()\n",
    "    for i,value in enumerator(values):\n",
    "        df.ix[df[category]==value, category] = i\n",
    "    df[category].astype(int, inplace=True)\n",
    "    return df\n",
    "\n",
    "def batch_process_categories(df, categories):\n",
    "    '''convert categorical features to numerical by batch, \n",
    "    return a dictionary of dictionaries storing the mapping of categorical value to number'''\n",
    "    cate_dict = {}\n",
    "    for category in categories:\n",
    "        '''Convert the categoricl column to numerical'''\n",
    "        if  category in df.columns.values:\n",
    "            cate_val = get_sorted_category_value(df,category) \n",
    "            '''The category_to_numer_dict() modify the input dataframe by side-effect and return a dictionary'''\n",
    "            cate_dict[category] = category_to_numer_dict(df, category, cate_val)\n",
    "    return cate_dict\n",
    "\n",
    "def check_group_mean(df, groupby_cols, target_cols):\n",
    "    for col in groupby_cols:\n",
    "        dfm = df.groupby(col).mean()\n",
    "        print dfm[target_cols]\n",
    "    return\n",
    "\n",
    "def check_group_stats(df, groupby_cols, target_cols):\n",
    "    for col in groupby_cols:\n",
    "        dfm = df.groupby(col).describe()\n",
    "        print dfm[target_cols]\n",
    "    return\n",
    "'''$$$$$$$$$$$$$$$$$$$$ Finishing EDA $$$$$$$$$$$$$$$$$$$$'''\n",
    "\n",
    "'''$$$$$$$$$$$$$$$$$$$$ End importing data $$$$$$$$$$$$$$$$$$$$'''\n",
    "def clean_data(dft):\n",
    "    '''remove and save the cases that are not closed'''\n",
    "    dft_still_open = dft[dft['Closed'].isnull()] # cases that not closed\n",
    "    filename_open = 'SF311_still_open_raw.csv'\n",
    "    dft_still_open_csv_path = folder + filename_open\n",
    "    dft_still_open.to_csv(dft_still_open_csv_path) # dft_still_open.csv contains cases that not closed\n",
    "\n",
    "    '''calculate the process time '''\n",
    "    condition = dft['Closed'].notnull()\n",
    "    dft_closed = dft[condition] # cases that are closed \n",
    "    dft_closed['Process_days'] = dft_closed['Closed'] - dft_closed['Opened']\n",
    "    dft_closed['Process_hours'] = dft_closed['Process_days'].apply(days_to_hours)\n",
    "\n",
    "    ''' remove cases with process time <= 0 hours and save theses cases '''\n",
    "    dft_wrong_dates = dft_closed[dft_closed['Process_hours'] <= 0]\n",
    "    filename_wrong_dates = 'SF311_wrong_dates_raw.csv'\n",
    "    dft_wrong_dates_csv_path = folder + filename_wrong_dates\n",
    "    dft_wrong_dates.to_csv(dft_wrong_dates_csv_path) \n",
    "    # dft_wrong_dates_raw.csv contains cases that have wrong dates: closed before opened'''re\n",
    "    dft_right_dates = dft_closed[dft_closed['Process_hours'] > 0]\n",
    "\n",
    "    ''' remove duplicated cases and save theses cases '''\n",
    "    dft_duplicates, dft_valid = check_word_in_col(dft_right_dates, 'Status Notes', 'Duplicate')\n",
    "    filename_duplicates = 'SF311_duplicates_raw.csv'\n",
    "    dft_duplicates_csv_path = folder + filename_duplicates\n",
    "    dft_duplicates.to_csv(dft_duplicates_csv_path) # dft_duplicates_raw.csv contains cases that are duplicated\n",
    "    \n",
    "    '''convert Process_days to float'''\n",
    "    dft_valid['Process_days'] = dft_valid['Process_hours']/24.0\n",
    "    \n",
    "    '''save raw valid cases'''\n",
    "    filename_valid = 'SF311_valid_raw.csv'\n",
    "    dft_valid_csv_path = folder + filename_valid\n",
    "    dft_valid.to_csv(dft_valid_csv_path)\n",
    "    \n",
    "    '''remove unnecessary columns and save the cases to csv file'''\n",
    "    drop_col = ['Updated','Status', 'Media URL']\n",
    "    dft_valid_reduced = dft_valid.drop(drop_col, axis =1)\n",
    "    filename_reduced = 'SF311_valid_reduced.csv'\n",
    "    dft_valid_reduced_csv_path = folder + filename_reduced\n",
    "    dft_valid_reduced.to_csv(dft_valid_reduced_csv_path)\n",
    "    \n",
    "    print 'Number of original cases: ', len(dft)\n",
    "    print 'Cases that are not closed: ', len(dft_still_open)\n",
    "    print 'Cases with process time <= 0: ', len(dft_wrong_dates)\n",
    "    print 'Cases with process time > 0: ', len(dft_right_dates)\n",
    "    print 'Duplicated cases: ', len(dft_duplicates)\n",
    "    print 'Valid cases: ', len(dft_valid)\n",
    "    return dft_valid_reduced\n",
    "\n",
    "\n",
    "def plot_data_on_date(df, data_col, year = False, month = False, day = False, dot = True):\n",
    "    ''' set index to date and plot df column data against the index, year can be Boolean or int'''\n",
    "    dfcp = df.copy()\n",
    "    dfcp.set_index('Opened', inplace = True)\n",
    "    if (type(year)==int) & (type(month)==int) & (type(day)==int):\n",
    "        cond1 = dfcp.index.year == year\n",
    "        cond2 = dfcp.index.month == month\n",
    "        cond3 = dfcp.index.day == day\n",
    "        dfcp1 = dfcp[cond1 & cond2 & cond3]           \n",
    "    elif (type(year)==int) & (type(month)==int):\n",
    "        cond1 = dfcp.index.year == year\n",
    "        cond2 = dfcp.index.month == month\n",
    "        dfcp1 = dfcp[cond1 & cond2]        \n",
    "    elif type(year)==int:\n",
    "        dfcp1 = dfcp[dfcp.index.year == year]\n",
    "    else:\n",
    "        dfcp1 = dfcp\n",
    "    if dot:     \n",
    "        dfcp1[data_col].plot(figsize=(18,16), c='m', alpha = 0.2,style='o')\n",
    "    else:\n",
    "        dfcp1[data_col].plot(figsize=(18,16), c='k', alpha = 0.2)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "'''This function is not necessary, becasue plot_data_on_date() can do scatter plot'''\n",
    "'''plot a scatter plot on data'''\n",
    "def scatter_data_on_date(df, data_col, year = False, month = False, day = False):\n",
    "    ''' set index to date and plot df column data against the index; year, month, and day can be Boolean or int'''\n",
    "    dfcp = df.copy()\n",
    "    dfcp['Opened_Int'] = dfcp['Opened'].astype(np.int64)\n",
    "    dfcp.set_index('Opened', inplace = True)\n",
    "    if (type(year)==int) & (type(month)==int) & (type(day)==int):\n",
    "        cond1 = dfcp.index.year == year\n",
    "        cond2 = dfcp.index.month == month\n",
    "        cond3 = dfcp.index.day == day\n",
    "        dfcp1 = dfcp[cond1 & cond2 & cond3]                   \n",
    "    elif (type(year)==int) & (type(month)==int):\n",
    "        cond1 = dfcp.index.year == year\n",
    "        cond2 = dfcp.index.month == month\n",
    "        dfcp1 = dfcp[cond1 & cond2]        \n",
    "    elif type(year)==int:\n",
    "        dfcp1 = dfcp[dfcp.index.year == year]        \n",
    "    else:\n",
    "        dfcp1 = dfcp\n",
    "    dfcp1.plot(kind = 'scatter', x='Opened_Int', y='Process_days', alpha = 0.2, c = 'm', figsize=(20,10))  \n",
    "    return\n",
    "\n",
    "\n",
    "'''######## this can be a short lambda expression########'''\n",
    "def get_str_list(string):\n",
    "    '''convert to a list of string'''\n",
    "    return str(string).split()\n",
    "\n",
    "'''######## this can be a short lambda expression########'''\n",
    "def check_dup(str_list):\n",
    "    '''check if word 'Duplicate' is in the string list'''\n",
    "    return 'Duplicate' in str_list\n",
    "\n",
    "def check_word_in_col(df, column, word):\n",
    "    '''check if a word in the column, returns a tuple of dataframes, \n",
    "    the first one contains the word and second one does not''' \n",
    "    df1 = df.copy()\n",
    "    get_str_list = lambda x: str(x).split()\n",
    "    check_dup = lambda x: word in x\n",
    "\n",
    "    df1[column+'1'] = df1[column].apply(get_str_list) # turn df1[column] into a list of strings\n",
    "    cond = df1[column+'1'].apply(check_dup) # check if df1[column+'1'] contains the word\n",
    "    df_found = df[cond]\n",
    "    df_not_found = df[~cond]\n",
    "    return df_found, df_not_found\n",
    "\n",
    "def add_features(df):\n",
    "    df1 = df.copy()\n",
    "    '''Add features: Day of week, Month, Year, Weekend '''\n",
    "    df1['Day_Of_Week'] = df1['Opened'].dt.dayofweek\n",
    "    df1['Month'] = df1['Opened'].dt.month\n",
    "    df1['Year'] = df1['Opened'].dt.year\n",
    "    \n",
    "    df1['Weekend'] = (df1['Day_Of_Week'].isin((5,6))).astype(int) # if the open day is at weekend\n",
    "    '''Add feature Holiday and Before_Holiday'''\n",
    "    df1['Opened_Int'] = df1['Opened'].astype(np.int64)\n",
    "    \n",
    "    cal = calendar()\n",
    "    holidays = cal.holidays()\n",
    "    df1['Holiday'] = ((df1['Opened'].dt.date).astype('datetime64').isin(holidays)).astype(int)\n",
    "    df1['Before_Holiday'] = (((df1['Opened'].dt.date).astype('datetime64')+timedelta(days = 1))\\\n",
    "                             .isin(holidays)).astype(int)\n",
    "    num_of_holiday = len(df1[df1['Holiday'] == 1])\n",
    "    num_of_before_holiday = len(df1[df1['Before_Holiday'] == 1])\n",
    "    return df1\n",
    "\n",
    "def get_oneway_anova(df, target_col, group_col, group_list=False):\n",
    "    if group_list:\n",
    "        groups = group_list\n",
    "    else:\n",
    "        groups = list(df[group_col].unique())\n",
    "    datasets =[]\n",
    "    for component in groups:\n",
    "        df_temp = df[df[group_col] == component]\n",
    "        datasets.append(np.array(df_temp[target_col]))\n",
    "    f_val, p_val = stats.f_oneway(*datasets)  \n",
    "    print \"One-way ANOVA P =\", p_val  \n",
    "    return p_val\n",
    "\n",
    "def add_current_open(df):\n",
    "    '''add column Current_Open which has the number of current open cases'''\n",
    "    def get_open_cases(opt):\n",
    "        condition1 = df['Opened'] < opt\n",
    "        condition2 = df['Closed'] > opt\n",
    "        open_cases = df[condition1 & condition2]\n",
    "        return len(open_cases)\n",
    "    df['Current_Open'] = df['Opened'].apply(get_open_cases)\n",
    "    return df\n",
    "\n",
    "def create_pilot(df, folder, filename):    \n",
    "    '''Create a pilot dataset of most recent 100000 cases for preliminary \n",
    "    modeling and feature engineering, save as SF311_pilot.csv '''\n",
    "    pilot = df.iloc[:100000,:]\n",
    "    pilot.to_csv(folder+filename_pilot)\n",
    "    return\n",
    "\n",
    "def impute_neighbor_knn():\n",
    "    pass\n",
    "\n",
    "def creat_data_chunk(df, folder, filename, days = 730):\n",
    "    if timedelta(days = days) > df['Opened'].max()-df['Opened'].min():\n",
    "        print 'Error! Days over the limit!'\n",
    "        return\n",
    "    new_end_date = df['Opened'].max() - timedelta(days = days) # get the date that is 2 year before the data collecting date\n",
    "    condition1 = df['Opened'] <= new_end_date\n",
    "    condition2 = df['Process_days'] <= days\n",
    "    df_chunk = df[condition1 & condition2]\n",
    "    df_chunk.to_csv(folder+filename)\n",
    "    return\n",
    "\n",
    "def train_test_df_split(df, test_size = 0.2, random_seed = 111):\n",
    "    np.random.seed(seed = random_seed)\n",
    "    df['Flag'] = np.random.random(size = len(df)) <= test_size\n",
    "    df_train = df[~df['Flag']]\n",
    "    df_test = df[df['Flag']]\n",
    "    df_train.drop('Flag', axis=1, inplace = True)\n",
    "    df_test.drop('Flag', axis=1, inplace = True)\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP precess\n",
    "\n",
    "1. get df from training dataset\n",
    "\n",
    "2. split it to train and validation sets\n",
    "\n",
    "3. use train set fit_transform tfidf vectorizer, save the matrix of tfidfed and the tfidf model in pickle\n",
    "\n",
    "4. use tfidfed to do kmeans lcustering and get the lable for each sample and save the centroids in pickle\n",
    "\n",
    "5. use that lable as request topic for training the model\n",
    "\n",
    "6. for validation set, first calculate the similarity of each sample's request type to the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "*****************************************************************\n",
    "From here: can be skipped because the valid cases have been stored in csv file\n",
    "*****************************************************************\n",
    "'''\n",
    "folder = '/Users/haowei/Documents/GN/Capstone/Capstone-project/data/'\n",
    "filename_original = 'SF311.csv'\n",
    "dft = import_data(folder, filename_original)\n",
    "dft_valid_reduced = clean_data(dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced dataframe shape:  (1935020, 14)\n"
     ]
    }
   ],
   "source": [
    "'''Read data from new cleaned and reduced csv file'''\n",
    "folder = '/Users/haowei/Documents/GN/Capstone/Capstone-project/data/'\n",
    "filename_reduced = 'SF311_valid_reduced.csv'\n",
    "\n",
    "cfdf = get_prep_data(folder+filename_reduced)\n",
    "cfdf_cp = cfdf.copy()\n",
    "print 'reduced dataframe shape: ', cfdf.shape\n",
    "\n",
    "cfdf = add_features(cfdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "*****************************************************************\n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "*****************************************************************\n",
    "'''\n",
    "'''Read data from new engineered csv file'''\n",
    "folder = '/Users/haowei/Documents/GN/Capstone/Capstone-project/data/'\n",
    "filename_engineered = 'SF311_engineered.csv'\n",
    "\n",
    "df = get_prep_data(folder+filename_engineered)\n",
    "df_cp = df.copy()\n",
    "\n",
    "# print 'Engineered dataframe info: ', df.info()\n",
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Opened_Int'] = df['Opened'].astype(float)\n",
    "filename_engineered = 'SF311_engineered.csv'\n",
    "df.to_csv(folder+filename_engineered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Read data from new filled csv file'''\n",
    "folder = '/Users/haowei/Documents/GN/Capstone/Capstone-project/data/'\n",
    "filename_fill = 'SF311_fill.csv'\n",
    "\n",
    "df_fill = get_prep_data(folder+filename_fill)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548369 386651 0.199817572945 1935020\n"
     ]
    }
   ],
   "source": [
    "'''Will use KNN to impute the neighborhood, but before doing it need to split the data into train-valid and test set'''\n",
    "'''Because KNN will use information from the whole dataset, lead to a data leakage'''\n",
    "'''Do a 80-20% train-test split on dataframe'''\n",
    "np.random.seed(seed = 111)\n",
    "df_fill['Flag'] = np.random.random(size = len(df_fill)) >=0.8\n",
    "df_train = df[~df['Flag']]\n",
    "df_test = df[df['Flag']]\n",
    "print len(df_train), len(df_test), len(df_test)*1./len(df), len(df_train)+ len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''write the train and test datasets to csv'''\n",
    "folder = '/Users/haowei/Documents/GN/Capstone/Capstone-project/data/'\n",
    "filename_train = 'SF311_train.csv'\n",
    "filename_test = 'SF311_test.csv'\n",
    "df_train.to_csv(folder+filename_train)\n",
    "df_test.to_csv(folder+filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Opened</th>\n",
       "      <th>Closed</th>\n",
       "      <th>Status Notes</th>\n",
       "      <th>Responsible Agency</th>\n",
       "      <th>Category</th>\n",
       "      <th>Request Type</th>\n",
       "      <th>Request Details</th>\n",
       "      <th>Address</th>\n",
       "      <th>Supervisor District</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>...</th>\n",
       "      <th>Process_days</th>\n",
       "      <th>Process_hours</th>\n",
       "      <th>Day_Of_Week</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Holiday</th>\n",
       "      <th>Before_Holiday</th>\n",
       "      <th>Opened_Int</th>\n",
       "      <th>Flag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CaseID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185580</th>\n",
       "      <td>2008-03-20 11:48:32</td>\n",
       "      <td>2013-03-26 18:13:25</td>\n",
       "      <td>Case Completed - resolved:</td>\n",
       "      <td>DPW Ops Queue</td>\n",
       "      <td>Tree Maintenance</td>\n",
       "      <td>Trees - Damaging_Property</td>\n",
       "      <td>Lifted_sidewalk_tree_roots</td>\n",
       "      <td>Intersection of FILLMORE ST and TURK ST</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Western Addition</td>\n",
       "      <td>...</td>\n",
       "      <td>1832.266667</td>\n",
       "      <td>43974.4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1206013712000000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196699</th>\n",
       "      <td>2008-04-13 14:04:28</td>\n",
       "      <td>2015-06-11 12:00:00</td>\n",
       "      <td>Case Completed - resolved:  Request closed by ...</td>\n",
       "      <td>DPW Ops Queue</td>\n",
       "      <td>Tree Maintenance</td>\n",
       "      <td>Trees - Damaging_Property</td>\n",
       "      <td>Lifted_sidewalk_tree_roots</td>\n",
       "      <td>479 GOLD MINE DR, SAN FRANCISCO, CA, 94131</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Diamond Heights</td>\n",
       "      <td>...</td>\n",
       "      <td>2614.912500</td>\n",
       "      <td>62757.9</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1208095468000000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Opened              Closed  \\\n",
       "CaseID                                           \n",
       "185580 2008-03-20 11:48:32 2013-03-26 18:13:25   \n",
       "196699 2008-04-13 14:04:28 2015-06-11 12:00:00   \n",
       "\n",
       "                                             Status Notes Responsible Agency  \\\n",
       "CaseID                                                                         \n",
       "185580                         Case Completed - resolved:      DPW Ops Queue   \n",
       "196699  Case Completed - resolved:  Request closed by ...      DPW Ops Queue   \n",
       "\n",
       "                Category               Request Type  \\\n",
       "CaseID                                                \n",
       "185580  Tree Maintenance  Trees - Damaging_Property   \n",
       "196699  Tree Maintenance  Trees - Damaging_Property   \n",
       "\n",
       "                   Request Details  \\\n",
       "CaseID                               \n",
       "185580  Lifted_sidewalk_tree_roots   \n",
       "196699  Lifted_sidewalk_tree_roots   \n",
       "\n",
       "                                           Address  Supervisor District  \\\n",
       "CaseID                                                                    \n",
       "185580     Intersection of FILLMORE ST and TURK ST                  5.0   \n",
       "196699  479 GOLD MINE DR, SAN FRANCISCO, CA, 94131                  8.0   \n",
       "\n",
       "            Neighborhood  ...   Process_days Process_hours  Day_Of_Week  \\\n",
       "CaseID                    ...                                             \n",
       "185580  Western Addition  ...    1832.266667       43974.4            3   \n",
       "196699   Diamond Heights  ...    2614.912500       62757.9            6   \n",
       "\n",
       "        Month  Year  Weekend  Holiday  Before_Holiday           Opened_Int  \\\n",
       "CaseID                                                                       \n",
       "185580      3  2008        0        0               0  1206013712000000000   \n",
       "196699      4  2008        1        0               0  1208095468000000000   \n",
       "\n",
       "         Flag  \n",
       "CaseID         \n",
       "185580  False  \n",
       "196699  False  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Read data from new train and test csv file'''\n",
    "folder = '/Users/haowei/Documents/GN/Capstone/Capstone-project/data/'\n",
    "filename_train = 'SF311_train.csv'\n",
    "filename_test = 'SF311_test.csv'\n",
    "\n",
    "df_tr = get_prep_data(folder+filename_train)\n",
    "# df_te = get_prep_data(folder+filename_test)\n",
    "# print len(df_tr), len(df_te)\n",
    "df_tr.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get df from pickle\n",
      "dataframe shape:  (1549243, 22)\n",
      "['trees - damaging_property', 'trees - damaging_property', 'trees - damaging_property', 'trees - damaging_property', 'abandoned vehicle - car4door']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haowei/anaconda/envs/py27/lib/python2.7/site-packages/ipykernel/__main__.py:345: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/haowei/anaconda/envs/py27/lib/python2.7/site-packages/ipykernel/__main__.py:348: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/haowei/anaconda/envs/py27/lib/python2.7/site-packages/ipykernel/__main__.py:349: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "'''Deal with request type using tf-idf then clustering'''\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from PremodelingProcess import train_vali_split, get_df_for_modeling, \\\n",
    "dump_object_to_pickle, get_df_for_engineer, process_data_for_survival_model\n",
    "\n",
    "\n",
    "'''Check if there is pickle files of dataframe ready for load'''\n",
    "filename_train_pickle = '../data/SF311_train.pickle'\n",
    "filename_train = '../data/SF311_train.csv'\n",
    "df = get_df_for_engineer(filename_train_pickle, filename_train)\n",
    "print 'dataframe shape: ', df.shape\n",
    "# print df.head()\n",
    "\n",
    "#df['Request Type'] = df['Request Type'].apply(lambda x: str(x).lower())\n",
    "df1 = df[:1000]#run a pilot\n",
    "df_tra, df_val = train_test_df_split(df1, test_size = 0.2, random_seed = 222)\n",
    "series_tra = df_tra['Request Type']\n",
    "series_val = df_val['Request Type']\n",
    "documents_train = list(series_tra)\n",
    "documents_validation = list(series_val)\n",
    "print documents_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize and remove stop words\n",
    "\n",
    "# 1. Create a set of documents.\n",
    "#documents = [' '.join(article['content']).lower() for article in coll.find()]\n",
    "\n",
    "# 2. Create a set of tokenized documents. No need to tokenize because there is no punctuation in the phrase\n",
    "# docs = [word_tokenize(content) for content in documents]\n",
    "# print docs[:15]\n",
    "\n",
    "# # 3. Strip out stop words from each tokenized document.\n",
    "# stop = set(stopwords.words('english'))\n",
    "# docs = [[word for word in words if word not in stop] for words in docs]\n",
    "\n",
    "# Stemming / Lemmatization\n",
    "\n",
    "# 1. Stem using both stemmers and the lemmatizer\n",
    "# porter = PorterStemmer()\n",
    "\n",
    "# snowball = SnowballStemmer('english')\n",
    "# wordnet = WordNetLemmatizer()\n",
    "# docs_porter = [[porter.stem(word) for word in words] for words in documents]\n",
    "\n",
    "# docs_snowball = [[snowball.stem(word) for word in words] for words in docs]\n",
    "# docs_wordnet = [[wordnet.lemmatize(word) for word in words] for words in docs]\n",
    "\n",
    "#print docs_porter[:30]\n",
    "\n",
    "# 3. Create word count vector over the whole corpus.\n",
    "# cv = CountVectorizer(stop_words='english')\n",
    "# vectorized = cv.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done tfidf\n"
     ]
    }
   ],
   "source": [
    "'''Make tfidf model and tfidfed matrix'''\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidfed = tfidf.fit_transform(documents_train)\n",
    "\n",
    "#print tfidfed\n",
    "'''save ftidf model to pickle file, will be used to transform the text in test file'''\n",
    "filename_tfidf_pickle = '../data/SF311_tfidf.pickle'\n",
    "filename_tfidfed_pickle = '../data/SF311_tfidfed.pickle'\n",
    "dump_object_to_pickle(tfidf,filename_tfidf_pickle)\n",
    "dump_object_to_pickle(tfidfed,filename_tfidfed_pickle)\n",
    "print 'done tfidf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(807, 66)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = tfidfed.todense()\n",
    "dense.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import cluster, datasets\n",
    "\n",
    "\n",
    "k_means = KMeans(n_clusters=10, n_jobs=-2)\n",
    "k_means.fit(tfidfed) \n",
    "print len(k_means.labels_)\n",
    "#print k_means.cluster_centers_[:5]\n",
    "print type(k_means.cluster_centers_)\n",
    "centers = k_means.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haowei/anaconda/envs/py27/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "df_tra['kmeans'] = k_means.labels_\n",
    "#df_tra.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['illegal_dumping', 'trees - damaged_tree', 'not_offensive graffiti on private property', 'overflowing_city_receptacle_or_dumpster', 'abandoned vehicle - pickuptruck']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(193, 66)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# def predict_kmeans_label(tfidfed, cluster_centers ):\n",
    "#     labels=[]\n",
    "#     for row in tfidfed:\n",
    "#         for center in cluster_centers:\n",
    "#             min_dis = \n",
    "print documents_validation[:5]\n",
    "valid_tfidfed = tfidf.transform(documents_validation)\n",
    "valid_dense = valid_tfidfed.todense()\n",
    "valid_dense.shape      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193,)\n",
      "[1 9 3 9 0 2 1 1 9 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.12727144],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.79774927,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.05031447],\n",
       "       [ 0.53790706,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.09433962],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.01257862]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities = linear_kernel(valid_dense, centers)\n",
    "cosine_similarities.shape\n",
    "type(cosine_similarities)\n",
    "validation_labels = np.argmax(cosine_similarities, axis =1)\n",
    "print validation_labels.shape\n",
    "print validation_labels[:10]\n",
    "cosine_similarities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
